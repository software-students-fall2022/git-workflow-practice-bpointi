# [Twitter pranksters derail GPT-3 bot with newly discovered “prompt injection” hack](https://arstechnica.com/information-technology/2022/09/twitter-pranksters-derail-gpt-3-bot-with-newly-discovered-prompt-injection-hack/?comments=1)

# Discussion

This article covers humorous incidences of Twitter users exploiting a recently-discovered and inherent vulnerability in the GPT-3 language model AI to make a Twitter bot say things outside the domain of what it is programmed to (you can visit the bot's twitter @remoteli.io). The technique has been coined "prompt injection" and is similar to an SQL injection in that the program erroneously executes a piece of user input that it shouldn't have. The difference between the two is that the natural language structure of the AI prohibits developers from remedying injection exploit risks through syntax fixes, as in traditional software. Time will tell if this exploit can dangerously grow beyond unintended outputs and if anything can be done in terms of its prevention.

***

<Mark>This is interesting<Mark> because it shows that the vulnerability is ubiquitous even though it won't create any actually damage. However, whether this will be exploited and lead to potential damage is unknown.
>Yvonne Wu
